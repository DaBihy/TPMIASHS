{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. **Import the necessary libraries and modules:**\n",
        "   - TensorFlow and Keras\n",
        "   - NumPy\n",
        "   - IMDB dataset and sequence padding utility\n",
        "   - Layers, Model, and Optimizer from Keras\n",
        "\n",
        "2. **Load the IMDB dataset:**\n",
        "   - Set the maximum number of words to keep (`max_features`)\n",
        "   - Set the maximum length of sequences (`maxlen`)\n",
        "   - Load and preprocess the data (padding sequences)\n",
        "\n",
        "3. **Implement the PositionalEmbedding class:**\n",
        "   - Initialize with the maximum sequence length and embedding dimension\n",
        "   - Calculate the positional embeddings using sine and cosine functions\n",
        "   - Add the positional embeddings to the input tokens in the `call` method\n",
        "\n",
        "4. **Implement the TokenAndPositionEmbedding class:**\n",
        "   - Initialize with the maximum sequence length, vocabulary size, and embedding dimension\n",
        "   - Use an Embedding layer for token embeddings and the custom PositionalEmbedding layer for position embeddings\n",
        "   - Add the token and position embeddings in the `call` method\n",
        "\n",
        "5. **Implement the MultiHeadAttention class:**\n",
        "   - Initialize with the model dimension and the number of heads\n",
        "   - Create weight matrices for query, key, and value projections\n",
        "   - Implement the `split_heads` method to reshape the input tensors\n",
        "   - Implement the `call` method to compute the scaled dot-product attention and output\n",
        "\n",
        "6. **Implement the TransformerEncoderLayer class:**\n",
        "   - Initialize with the model dimension, number of heads, feed-forward hidden layer dimension, and dropout rate\n",
        "   - Use the custom MultiHeadAttention layer and a feed-forward neural network for the self-attention and position-wise feed-forward operations\n",
        "   - Implement layer normalization and dropout layers\n",
        "   - Implement the `call` method to compute the output of the encoder layer\n",
        "\n",
        "7. **Build the Transformer model:**\n",
        "   - Set the embedding dimension, number of heads, feed-forward hidden layer dimension, and number of layers\n",
        "   - Create the input layer\n",
        "   - Use the custom TokenAndPositionEmbedding layer for input embeddings\n",
        "   - Add padding mask for input sequences\n",
        "   - Stack the TransformerEncoderLayer layers\n",
        "   - Add the output layers (GlobalAveragePooling1D, Dense, and Dropout)\n",
        "\n",
        "8. **Compile and train the model:**\n",
        "   - Define a learning rate scheduler\n",
        "   - Create a callback for the scheduler\n",
        "   - Instantiate an optimizer with the initial learning rate\n",
        "   - Compile the model with the optimizer, binary_crossentropy loss, and accuracy metric\n",
        "   - Train the model using the training data, batch size, epochs, validation split, and callback\n"
      ],
      "metadata": {
        "id": "FR14UGjdDuP9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7uDwbavRYU7d"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. **Load the IMDB dataset:**\n"
      ],
      "metadata": {
        "id": "EVMh-090Dz1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the IMDB dataset\n",
        "max_features = 20000  # Maximum number of words to keep\n",
        "maxlen = 200  # Maximum length of sequences\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "# Pad the sequences to the same length\n",
        "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = pad_sequences(x_test, maxlen=maxlen)\n"
      ],
      "metadata": {
        "id": "_peNFbx9Ysz9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. **Implement the PositionalEmbedding class:**\n",
        "\n",
        "- Initialize with the maximum sequence length and embedding dimension\n",
        "- Calculate the positional embeddings using sine and cosine functions\n",
        "- Add the positional embeddings to the input tokens in the `call` method"
      ],
      "metadata": {
        "id": "WwnCwVy3D5Gv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, max_len, embed_dim):\n",
        "        super().__init__()\n",
        "        self.positional_embedding = self.build_positional_embedding(max_len, embed_dim)\n",
        "\n",
        "    def build_positional_embedding(self, max_len, embed_dim):\n",
        "        angle_rates = 1 / np.power(10000, (2 * (np.arange(embed_dim) // 2)) / np.float32(embed_dim))\n",
        "        angle_rads = np.arange(max_len)[:, np.newaxis] * angle_rates[np.newaxis, :]\n",
        "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "        pos_embedding = angle_rads[np.newaxis, ...]\n",
        "        return tf.cast(pos_embedding, dtype=tf.float32)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        seq_len = tf.shape(inputs)[-2]\n",
        "        return inputs + self.positional_embedding[:, :seq_len, :]\n",
        "\n"
      ],
      "metadata": {
        "id": "-yI7A7hGYvYQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. **Implement the TokenAndPositionEmbedding class:**\n",
        "   - Initialize with the maximum sequence length, vocabulary size, and embedding dimension\n",
        "   - Use an Embedding layer for token embeddings and the custom PositionalEmbedding layer for position embeddings\n",
        "   - Add the token and position embeddings in the `call` method\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eHvi-rdnES8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = PositionalEmbedding(max_len=maxlen, embed_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.token_emb(x)\n",
        "        return self.pos_emb(x)\n"
      ],
      "metadata": {
        "id": "jN5LekGQESvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. **Implement the MultiHeadAttention class:**\n",
        "   - Initialize with the model dimension and the number of heads\n",
        "   - Create weight matrices for query, key, and value projections\n",
        "   - Implement the `split_heads` method to reshape the input tensors\n",
        "   - Implement the `call` method to compute the scaled dot-product attention and output\n",
        "\n"
      ],
      "metadata": {
        "id": "fhvZYorDEhzQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the MultiHeadAttention class, which inherits from tf.keras.layers.Layer\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    # Initialize the class with the model dimension and the number of attention heads as input arguments\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        # Call the parent class constructor\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        # Store the number of attention heads\n",
        "        self.num_heads = num_heads\n",
        "        # Store the model dimension\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Check that the model dimension is divisible by the number of attention heads\n",
        "        assert d_model % num_heads == 0\n",
        "\n",
        "        # Calculate the depth of each attention head\n",
        "        self.depth = d_model // num_heads\n",
        "\n",
        "        # Define the weight matrices for the query, key, and value projections\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        # Define the dense layer for the output\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    # Method to split the input tensor into multiple heads\n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    # Method to compute the multi-head attention for the input query, key, and value tensors\n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        # Apply the weight matrices to the query, key, and value tensors\n",
        "        q = self.wq(q)\n",
        "        k = self.wk(k)\n",
        "        v = self.wv(v)\n",
        "\n",
        "        # Split the input tensors into multiple heads\n",
        "        q = self.split_heads(q, batch_size)\n",
        "        k = self.split_heads(k, batch_size)\n",
        "        v = self.split_heads(v, batch_size)\n",
        "\n",
        "        # Compute the scaled dot-product attention\n",
        "        matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "        # Apply the mask, if provided\n",
        "        if mask is not None:\n",
        "            scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "        # Compute the attention weights using softmax\n",
        "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "        # Compute the weighted sum of the values\n",
        "        output = tf.matmul(attention_weights, v)\n",
        "\n",
        "        # Transpose and reshape the output tensor to match the input shape\n",
        "        output = tf.transpose(output, perm=[0, 2, 1, 3])\n",
        "        concat_attention = tf.reshape(output, (batch_size, -1, self.d_model))\n",
        "\n",
        "        # Apply the output dense layer\n",
        "        output = self.dense(concat_attention)\n",
        "\n",
        "        # Return the output tensor and the attention weights\n",
        "        return output, attention_weights\n"
      ],
      "metadata": {
        "id": "RYdbpoC1a-9B"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. **Implement the TransformerEncoderLayer class:**\n",
        "   - Initialize with the model dimension, number of heads, feed-forward hidden layer dimension, and dropout rate\n",
        "   - Use the custom MultiHeadAttention layer and a feed-forward neural network for the self-attention and position-wise feed-forward operations\n",
        "   - Implement layer normalization and dropout layers\n",
        "   - Implement the `call` method to compute the output of the encoder layer\n",
        "\n"
      ],
      "metadata": {
        "id": "p-F1ckF0Emak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(dff, activation='relu'),\n",
        "            tf.keras.layers.Dense(d_model)\n",
        "        ])\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        return out2\n"
      ],
      "metadata": {
        "id": "7BFqKsbSbMCe"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. **Build the Transformer model:**\n",
        "   - Set the embedding dimension, number of heads, feed-forward hidden layer dimension, and number of layers\n",
        "   - Create the input layer\n",
        "   - Use the custom TokenAndPositionEmbedding layer for input embeddings\n",
        "   - Add padding mask for input sequences\n",
        "   - Stack the TransformerEncoderLayer layers\n",
        "   - Add the output layers (GlobalAveragePooling1D, Dense, and Dropout)\n",
        "\n"
      ],
      "metadata": {
        "id": "A419Hfs6Ep8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 32  # Embedding size for each token\n",
        "num_heads = 2  # Number of attention heads\n",
        "ff_dim = 32 \n",
        "num_layers = 1\n",
        "\n",
        "inputs = layers.Input(shape=(maxlen,))\n",
        "embedding_layer = TokenAndPositionEmbedding(maxlen, max_features, embed_dim)\n",
        "x = embedding_layer(inputs)\n",
        "\n",
        "padding_mask = tf.cast(tf.math.equal(inputs, 0), tf.float32)[:, tf.newaxis, tf.newaxis, :]\n",
        "for _ in range(num_layers):\n",
        "    x = TransformerEncoderLayer(embed_dim, num_heads, ff_dim)(x, training=True, mask=None)\n",
        "\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "x = layers.Dense(32, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)\n"
      ],
      "metadata": {
        "id": "rQ16f6shbRTn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. **Compile and train the model:**\n",
        "   - Instantiate an optimizer with the initial learning rate\n",
        "   - Compile the model with the optimizer, binary_crossentropy loss, and accuracy metric\n",
        "   - Train the model using the training data, batch size, epochs, validation split, and callback"
      ],
      "metadata": {
        "id": "Pr--Vj0eEwSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a learning rate scheduler\n",
        "def scheduler(epoch, lr):\n",
        "    if epoch < 5:\n",
        "        return lr\n",
        "    else:\n",
        "        return lr * 0.9\n",
        "\n",
        "callback = LearningRateScheduler(scheduler)\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "history = model.fit(x_train, y_train, batch_size=64, epochs=5, validation_split=0.2, callbacks=[callback])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fi2JRIfLdT0w",
        "outputId": "76969b8d-cd9f-41e9-dc08-6bc89541e506"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "313/313 [==============================] - 65s 196ms/step - loss: 0.5987 - accuracy: 0.6319 - val_loss: 0.3683 - val_accuracy: 0.8388 - lr: 0.0010\n",
            "Epoch 2/5\n",
            "313/313 [==============================] - 69s 220ms/step - loss: 0.2617 - accuracy: 0.8947 - val_loss: 0.2904 - val_accuracy: 0.8834 - lr: 0.0010\n",
            "Epoch 3/5\n",
            "313/313 [==============================] - 67s 215ms/step - loss: 0.1706 - accuracy: 0.9373 - val_loss: 0.3077 - val_accuracy: 0.8750 - lr: 0.0010\n",
            "Epoch 4/5\n",
            "313/313 [==============================] - 65s 209ms/step - loss: 0.1008 - accuracy: 0.9667 - val_loss: 0.3574 - val_accuracy: 0.8780 - lr: 0.0010\n",
            "Epoch 5/5\n",
            "313/313 [==============================] - 66s 210ms/step - loss: 0.0582 - accuracy: 0.9822 - val_loss: 0.4202 - val_accuracy: 0.8744 - lr: 0.0010\n"
          ]
        }
      ]
    }
  ]
}